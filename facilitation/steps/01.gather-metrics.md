# Se faire une idée de la code base
Imagine que tu dois reprendre la maintenance évolutive du système de gestion des parties de chasse de nos chers chasseurs du bouchonnois.

Afin de te faire un premier feeling sur le code, on va analyser certains éléments factuels sur le code.

## Check-list
On dresse une check-list de choses à vérifier / analyser :
- [ ] Compiler afin de valider qu'on est en capacité de compiler / exécuter le code
  - Analyser les potentiels `Warning` à la compilation
- [ ] Analyser la structure de la solution afin de comprendre l'architecture en place
- [ ] Regarder quels packages sont référencés afin de comprendre les interactions potentielles du système

### Récolter des metrics
Nos outils et l'écosystème de développement nous permet de récolter rapidement des metrics permettant d'observer de manière assez factuelle la qualité du code :
- [ ] Récupérer le `code coverage` 
- [ ] Mettre en place un outil d'analyse static de code
- [ ] Utiliser un Linter
- [ ] Identifier s'il y a des [`hotspots`](https://understandlegacycode.com/blog/focus-refactoring-with-hotspots-analysis/) et où ils sont localisés
- [ ] Revue de la qualité des tests

## Compiler
Lancer une première compilation afin d'identifier de potentiels problèmes.
![Build warnings](img/01.gather-metrics/build-warnings.webp)

On a déjà une première boucle de feedback de la part de notre compilateur nous indiquant que certains `fields` sont déclarés comme `non-nullable` mais pas initialisés.

## Analyser la structure
La structure d'une solution / projet est un indicateur de l'architecture sous-jacente.
L'observer nous permet de comprendre a minima le design du système.

Pour en savoir plus : [Screaming Architecture](https://blog.cleancoder.com/uncle-bob/2011/09/30/Screaming-Architecture.html).

![Folder structure](img/01.gather-metrics/folder-structure.webp)

Ici on peut tout de suite comprendre plusieurs éléments :
- Bonne nouvelle !!! la solution comprend des tests automatisés
- Le projet `Bouchonnois` semble être organisé autour d'un `Domain Model`
  - Une abstraction de repository semble servir de `port`
  - Le service `PartieDeChasseService` doit être le point d'entrée du système
  - Que fait la classe `Terrain` dans le répertoir `Service` ?
- Il y a une gestion d'Exceptions métiers et non pas des exceptions génériques

## Analyser les références
On jette un oeil sur les packages référencés afin de comprendre si le système intéragit avec d'autres systèmes (via gRPC, REST API, Integration Bus, Cloud Services, ...)

![Installed packages](img/01.gather-metrics/installed-packages.webp)

A part les références au framework, aucune dépendance n'est "exotique".
Les seuls packages utilisés sont ceux pour les tests.

## Code Coverage
On lance nos tests en activant le `code coverage`:
![Code coverage](img/01.gather-metrics/coverage.webp)

Super nouvelle !!! on a du code source couvert à 100%.  
Est-ce que celà suffit pour nous rendre complètement confiant vis-à-vis de nos futurs refactorings ?

## Analyse static de code
Nous allons dès maintenant mettre en place un outil d'analyse static de code ([SonarCloud](https://www.sonarsource.com/products/sonarcloud/) ici) et automatiser son lancement via notre chaine de build.
Celà nous permettra tout au long de l'atelier de vérifier l'impact de nos améliorations sur le code.

En utilisant `github` on peut profiter des `github actions` pour ce faire:

```yaml
name: SonarCloud
on:
  push:
    branches:
      - steps/01-gather-metrics
jobs:
  build:
    name: Build and analyze
    runs-on: windows-latest
    steps:
      - name: Set up JDK 11
        uses: actions/setup-java@v3
        with:
          java-version: 11
          distribution: 'zulu' # Alternative distribution options are available.
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Shallow clones should be disabled for a better relevancy of analysis
      - name: Cache SonarCloud packages
        uses: actions/cache@v3
        with:
          path: ~\sonar\cache
          key: ${{ runner.os }}-sonar
          restore-keys: ${{ runner.os }}-sonar
      - name: Cache SonarCloud scanner
        id: cache-sonar-scanner
        uses: actions/cache@v3
        with:
          path: .\.sonar\scanner
          key: ${{ runner.os }}-sonar-scanner
          restore-keys: ${{ runner.os }}-sonar-scanner
      - name: Install SonarCloud scanner
        if: steps.cache-sonar-scanner.outputs.cache-hit != 'true'
        shell: powershell
        run: |
          New-Item -Path .\.sonar\scanner -ItemType Directory
          dotnet tool update dotnet-sonarscanner --tool-path .\.sonar\scanner
      - name: Build and analyze
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        shell: powershell
        run: |
          // On lance l'analyse 
          dotnet tool install --global dotnet-coverage
          .\.sonar\scanner\dotnet-sonarscanner begin /k:"ythirion_refactoring-du-bouchonnois" /o:"ythirion" /d:sonar.login="${{ secrets.SONAR_TOKEN }}" /d:sonar.host.url="https://sonarcloud.io" /d:sonar.cs.vscoveragexml.reportsPaths=coverage.xml
          dotnet build
          // On ajoute le rapport de coverage
          dotnet-coverage collect "dotnet test" -f xml -o "coverage.xml"
          .\.sonar\scanner\dotnet-sonarscanner end /d:sonar.login="${{ secrets.SONAR_TOKEN }}"
```



- Feeling Architecture (Screaming Architecture) = structure des fichiers
- Mettre en place SonarCloud
- Identifier les Hotspots avec Codescene
	- Prioriser les refactorings
	- Identifier où se trouve la complexité : la logique métier
- Ajout des badges sur le repo
- Revue de la qualité des tests
	- Pouvoir avoir confiance en nos tests
- Avoir 0 tolérance pour les warnings
	- Les fixer